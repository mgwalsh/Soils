---
title: Spectral predictions for diagnosing reserve soil acidity
author: M.G. Walsh
date: "`r format(Sys.time(), '%d, %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 1
    fig_caption: true
    css: style.css
---

```{r, echo=FALSE}
knitr::opts_chunk$set(message = FALSE)
```

# Introduction

Soil acidity can be thought of as occurring in two forms: as active or *"soil solution acidity"*, measured by pH in water, and as *"reserve acidity"*. The reserve (or exchangeable) acidity, measured by Hp (meq $H^+$/ 100 g soil), is held on soil colloids and controls the overall level of soluble or active acidity in the soil solution, and is generally many times higher than the active acidity. Hp levels depend on several properties such as the amount and type of clay ([mineralogy](https://en.wikipedia.org/wiki/Mineralogy)), the amount of organic matter, and the soluble Al, Ca, Mg, K and Na cation concentrations of the soil. Thus, two soils may have the same soil pH but completely different lime requirements. 

To effectively raise the soil pH over the long-term, both active and reserve acidity should be neutralized. Soil test labs typically determine the buffering capacity and the lime requirement by measuring or estimating reserve acidity; see e.g., the excellent introductory tutorial in [Moorberg & Crouse (v.2)](https://kstatelibraries.pressbooks.pub/soilslabmanual/chapter/soil-acidity-and-adjusting-soil-ph/). Although soil tests determine pH cheaply and rapidly, it is not possible to make an accurate lime recommendation based solely pH measurements.

Spectral signatures of soils, and materials generally, are defined by their reflectance or absorbance as a function of wavelength in the electromagnetic spectrum. Under controlled conditions, the signatures result from electronic transitions of atoms and vibrational stretching and bending of structural groups of atoms that form molecules or crystals. Mid-infrared (MIR) spectroscopy has been shown to provide highly repeatable, rapid and low cost measurements of many different soil properties in numerous studies. The amount of light absorbed by a soil sample is measured with minimal sample preparation across a wide range of wavelengths to provide a unique spectral signature. A measurement can be performed in about 30 seconds, in contrast to conventional soil tests, which are typically slow, labor-intensive, expensive and/or use harmful chemicals.

This notebook is intended for self-learning. It does not go into the details of [spectroscopy](https://en.wikipedia.org/wiki/Spectroscopy) itself, but instead it focuses on the **spectrometry**, that is the steps that are needed to generate useful predictions from a population of spectral signatures (features) relative to their corresponding reference measurements (labels). In this particular example, I shall use topsoil (0-20 cm) and subsoil (20-50 cm) data that were sampled as part of the AfSIS project which sampled the major [KÃ¶ppen-Geiger climate zones](http://koeppen-geiger.vu-wien.ac.at/) of Africa excluding deserts, urban and other non-photosynthetically active land areas. 

```{r training_validation_approach, echo=FALSE, fig.align="center", fig.cap="MLA training, validation and prediction workflow.", out.width = '80%'}
knitr::include_graphics("./Wetchem/Figures/training_validation.png")
```

I'll be using a machine learning approach, which is *algorithmic*, rather than *data modeling* based. You might want to take a look at [Breiman (2001)](http://staff.pubhealth.ku.dk/~tag/Teaching/share/material/Breiman-two-cultures.pdf) to gauge the differences between the *"two cultures"* he describes. The figure above shows the basic workflow of the algorithmic approach that I apply here to predicting reserve acidity (Hp) as an example. The main motivation for using Hp in this example is that AfSIS is currently involved in several new, large-area projects that are focusing on acid soil management and cropland lime requirements. The other operational management aspect of this is that acidity problems should probably be diagnosed and fixed first, in the areas where they occur, and before attempting to solve e.g., fertilizer input and/or other soil related, agronomic management issues.

# General data setup

To actually run the notebook, you will need to load the packages indicated in the chunk directly below. This allows you to assemble the wet chemistry and spectral dataframes providing a lot of options to generate spectral predictions of acidity relevant soil properties such as pH, EC, Hp, eCEC Ca:Mg, among others. The notebook itself is maintained on my [Github](https://github.com/mgwalsh/Soils/blob/master/Spec_acidity_preds.Rmd), and you can fork and modify it from there as you see fit.

```{r}
# Package names
packages <- c("downloader", "caret", "caretEnsemble", "MASS", "pls", "glmnet", "randomForest", "xgboost", "Cubist", "quantreg", "leaflet", "htmlwidgets", "plyr", "dplyr", "doParallel")

# Install packages
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# Load packages
invisible(lapply(packages, library, character.only = TRUE))
```

The next chunk downloads the data needed for running this particular example. It assembles georeferenced soil measurements collected across Africa by the Africa Soil Information Service (AfSIS) project and links these to [Bruker Alpha FT-IR](https://www.bruker.com/en/products-and-solutions/infrared-and-raman/ft-ir-routine-spectrometer/alpha-ii-compact-ft-ir-spectrometer.html) spectra. Note that this chunk is Mac or Linux specific and so the directory structures would need to be changed slightly to run on Windows machines.

```{r}
# Create a data folder in your current working directory
dir.create("soil_acidity", showWarnings = F)
setwd("./soil_acidity")
dir.create("Results", showWarnings = F)

# Data download
download("https://osf.io/f4dpw/?raw=1", "acidity_data.zip", mode="wb")
unzip("acidity_data.zip", overwrite = T)
prof <- read.table("profiles.csv", header=T, sep=",") ## sample locations and depths
cnls <- read.table("cnls.csv", header=T, sep=",") ## wet chemistry data from CNLS, Nairobi
cnls$eCEC <- cnls$m3Ca/200+cnls$m3Mg/120+cnls$m3K/390+cnls$m3Na/230 ## calculates eCEC  
text <- read.table("text.csv", header=T, sep=",") ## AfSIS LDPSA soil texture
oxid <- read.table("oxid.csv", header=T, sep=",") ## AfSIS XRF metal oxide data
spec <- read.table("alpha_spec.csv", header=T, sep=",") ## AfSIS spectral absorbance data
  
# Merge the wet chemistry reference dataframes
adata <- merge(prof, cnls, by = "ssid")
adata <- merge(adata, text, by = "ssid")
adata <- merge(adata, oxid, by = "ssid")

# Download figures
download("https://osf.io/42gry/", "figures.zip", mode = "wb")
exdir <- "./Figures" 
unzip("figures.zip", exdir = exdir, overwrite = T)
```

The following chunk then writes out the dataframe `AfSIS_reference_data.csv` into your `./soil_acidity/Results` directory if you'd like to process those outputs in software other than R. It also generates a location map of where those soil samples were obtained.

```{r}
# Write out reference data frame
write.csv(adata, "./soil_acidity/Results/AfSIS_reference_data.csv", row.names = F)

# Soil sample locations
w <- leaflet() %>%
  setView(lng = mean(adata$lon), lat = mean(adata$lat), zoom = 3) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>%
  addCircleMarkers(adata$lon, adata$lat, clusterOptions = markerClusterOptions())
w ## plot widget 
```

# Spectral feature conversions

One potentially problematic factor for many MLAs is the [*"Curse of dimensionality"*](https://en.wikipedia.org/wiki/Curse_of_dimensionality) that is imposed by the high dimensionality of the spectral data: *w* = 2,541 individual wavebands in this particular case. Moreover, many of those wavebands are strongly collinear (correlated), particularly those proximal to one another on the spectrum. These types of data pose generalization and prediction challenges for MLAs that involve [Bagging](https://en.wikipedia.org/wiki/Random_forest), [Boosting](https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29), [Bayesian](https://www.annualreviews.org/doi/pdf/10.1146/annurev-statistics-031219-041110) and [Deep learning](https://en.wikipedia.org/wiki/Deep_learning) and can lead to [overfitting](https://en.wikipedia.org/wiki/Overfitting) problems. 

Other algorithms such as [Partial least squares regression (PLS)](https://en.wikipedia.org/wiki/Partial_least_squares_regression), [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) and/or [Lasso regression](https://en.wikipedia.org/wiki/Lasso_(statistics)) handle the high dimensional data and collinearities well in a linear context but are frequently not very good at predicting the common non-linear and/or threshold relationships between the spectral signatures and their reference measurements.

There are some ways of mitigating these trade offs. The first is to reduce the dimensionality and collinearity of the spectral data. There are a number of pre-processing techniques that can be applied in that context including: [Principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis), [Independent components analysis (ICA)](https://en.wikipedia.org/wiki/Independent_component_analysis) as well as other signal processing techniques such as, [Non-negative matrix factorization (NMF)](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization). I use PCA here, and the next chunk appends the spectra and their resulting spectral component scores to the wet chemistry reference data.

```{r}
# Spectral principal components
spec.pca <- prcomp(spec[,2:2542], center=T, scale=T)
pcas <- predict(spec.pca, spec)
pcas <- pcas[,1:20] ## save the first 20 components, which explain ~95% of the total spectral variance
fname <- paste("./soil_acidity/Results/", "spec_pcas.rds", sep = "")
saveRDS(spec.pca, fname) ## saves spectral PCA model

# Merge & write files
spec <- cbind(spec, pcas)
adata <- merge(adata, spec, by = "ssid")
write.csv(adata, "./soil_acidity/Results/AfSIS_acidity_data.csv", row.names=F)
```

The other main option is of course to run both types of algorithms (the data regularization/data-selection-based and the data reduction-based), and then look at if they can be usefully combined. This is the approach that I will take here.

# Model training with `caretEnsemble`

The following chunks predict soil Hp values using different machine learning algorithms (MLAs) with MIR spectral (feature) inputs using the [`caretEnsemble`](https://cran.r-project.org/web/packages/caretEnsemble/index.html) package. This general approach has won a lot of data science competitions e.g., at [Kaggle](https://www.kaggle.com/). You may want to take a look there. They have some fantastic data science resources, courses and challenges openly available. You can also check out the AfSIS sponsored [Africa Soil Property Prediction Challenge (2014)](https://www.kaggle.com/c/afsis-soil-properties/overview/timeline), from which the notebook that is presented here was developed.

The main idea is to train a number of potentially competing MLA models with [k-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). At the end of the model training processes, the various models are ensembled (combined/stacked) on an *independent* validation dataset. When applied consistently over time and space, this is a form of [Reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning), which should produce increasingly accurate predictions as new field, lab data and MLAs are obtained and run. The following chunk initially scrubs some of the extraneous objects in memory, sets-up labels and features, and creates a randomized partition between the training and validation dataframes.

```{r}
rm(list=setdiff(ls(), c("adata"))) ## scrubs extraneous objects in memory

# Set randomization seed
seed <- 1235813
set.seed(seed)

# Randomize dataframe
adata <- adata[sample(1:nrow(adata)), ]

# Split data into calibration and validation sets
gsIndex <- createDataPartition(adata$Hp, p = 8/10, list=F, times = 1)
cal <- adata[ gsIndex,]
val <- adata[-gsIndex,]

# Set calibration labels
labs <- c("Hp") ## insert other labels ("pH", "EC", "eCEC", ...) here!
lcal <- as.vector(t(cal[labs]))

# Calibration features
fcal <- select(cal,26:2566,9) ## full Alpha FT-IR spectral signatures + pH
pcal <- select(cal,2567:2586,9) ## spectral principal components + pH
```

Note that I also include soil pH (in water) in the calibration features because it is routinely, quickly and cheaply measured in most soil spectral labs (and frequently even in the field). It is also closely related to Hp. The general notion is that soils with a pH > 7 are unlikely to have any measurable reserve acidity. See the figure below.

```{r, fig.align = "center", fig.cap = "Relationship between soil pH and reserve acidity (Hp)."}
par(pty="s", mar=c(4,4,1,1))
plot(Hp~pH, xlab="pH (water)", ylab="Hp (meq / 100 g soil)", cex.lab=1.3, 
     xlim=c(3,10), ylim=c(0,5), adata)
```

[**`caretEnsemble`**](https://cran.r-project.org/web/packages/caretEnsemble/caretEnsemble.pdf) has 3 primary functions: `caretList`, `caretEnsemble` and `caretStack`. `caretList` is used to build lists of caret models on the same training data, with the same model resampling parameters. `caretEnsemble` and `caretStack` are used to create ensemble models from such lists of individual `caret` models. `caretEnsemble` uses a generalized linear model [glm](https://en.wikipedia.org/wiki/Generalized_linear_model) to create a simple weighted combination of the constituent models and `caretStack` uses a specific caret model to combine the outputs from a choice of other component `caret` models. The main advantage of running `caretEnsemble` is that the intended individual [`caret`](https://topepo.github.io/caret/) models can be run *"all-at-once"*, rather than running each model separately ... abbreviating and hopefully clarifying the calibration code. All model fitting processes can be (are) parallelized to facilitate efficient use of either local or cloud-based computing resources. Note that there are also other options available for this e.g., the [`snowfall`](https://cran.r-project.org/web/packages/snowfall/snowfall.pdf) package, among others. 

The next chunk fits 3 initial models that use the full spectral signatures i.e., the *w* = 2,541 individual wavebands. These are reasonably common linear [Chemometric](https://en.wikipedia.org/wiki/Chemometrics) algorithms and you can learn more about how they work in R from the following links: [`pls`](https://www.rdocumentation.org/packages/mixOmics/versions/6.3.2/topics/pls), [`glmnet`](https://www.rdocumentation.org/packages/glmnet/versions/4.1-1/topics/glmnet) and [`xgbLinear`](https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r/). I fit these with 10-fold cross-validation and default-tuning of the relevant [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). Also, note that that this chunk can take up to 1 hour to run on a normal computer with 4 cores. There are just a lot of variables to sort through.

```{r, warning = FALSE}
# Start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# Specify model training controls
set.seed(seed)
tc <- trainControl(method = "cv", number = 10, allowParallel = TRUE, savePredictions="final")

# Fit the 3 initial calibration models
llist <- caretList(fcal, lcal,
                   trControl = tc,
                   tuneList = NULL,
                   methodList = c("pls", "glmnet", "xgbLinear"),
                   preProcess = c("center","scale"),
                   metric = "RMSE")
stopCluster(mc)
```

The next chunk fits 3 additional, tree/forest based MLAs that use the that use the spectral principal component scores (pc = 20). You can learn more about how they work in R from the following links at: [randomForest](https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest), [xgboost](https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r/)) and [Cubist](https://cran.r-project.org/web/packages/Cubist/Cubist.pdf)). This chunk runs quite quickly.

```{r, warning = FALSE}
# Start doParallel to parallelize model fitting
set.seed(seed)
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# Specify model training controls
tc <- trainControl(method = "cv", number = 10, allowParallel = TRUE, savePredictions="final")

# Fit 3 calibration models using the spectral principal component features
tlist <- caretList(pcal, lcal,
                   trControl = tc,
                   tuneList = NULL,
                   methodList = c("rf", "xgbTree", "cubist"),
                   preProcess = c("center","scale"),
                   metric = "RMSE")
stopCluster(mc)
```

# Model stacking and prediction with `caret`

The main point here is not to evaluate a *best individual model* but rather to evaluate the combination of the previously fitted models against a 20% [hold-out](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) validation dataset. This provides robust statistical estimates of how the different models should be weighted against one-another. This chunk initially generates the individual model predictions for the validation dataframe.

```{r, warning = FALSE}
# Validation features
fval <- select(val,26:2566,9) ## full Alpha FT-IR spectral signatures + pH
pval <- select(val,2567:2586,9) ## spectral principal components + pH

val$pls <- predict(llist$pls, fval)
val$glm <- predict(llist$glmnet, fval)
val$xbl <- predict(llist$xgbLinear, fval)
val$rfo <- predict(tlist$rf, pval)
val$xbt <- predict(tlist$xgbTree, pval)
val$cub <- predict(tlist$cubist, pval)
```

This next chunk fits the model ensemble with the `glmStepAIC` function from the `MASS` library using the **validation dataframe**. You could explore other options here, but I that that this provides a reasonable combination and weighting of the 6 models that were produced in the ensemble training steps.

```{r, results='hide'}
lval <- as.vector(t(val[labs]))
fval <- select(val,pls,glm,xbl,rfo,xbt,cub) ## subset validation models

# Start doParallel to parallelize model fitting
mc <- makeCluster(detectCores())
registerDoParallel(mc)

# Model setup and fitting
set.seed(seed)
tc <- trainControl(method="repeatedcv", number=10, repeats=3, allowParallel=T)

st <- train(fval, lval,
            method = "glmStepAIC",
            trControl = tc,
            metric = "RMSE")

val$st <- predict(st, val) ## stacked predictions
stopCluster(mc)
fname <- paste("./soil_acidity/Results/", labs, "_st.rds", sep = "")
saveRDS(st, fname)
```

```{r, echo = FALSE}
summary(st)
```

# Ensemble prediction uncertainty estimates

There are many ways to quantify the uncertainty inherent in these predictions. We take a simple but quite robust approach here using quantile regression with ([quantreg](https://cran.r-project.org/web/packages/quantreg/quantreg.pdf)). I am mainly interested in the overall spread of the spectral ensemble predictions (sensu, their 95% probable intervals for the entire dataset).

```{r, warning = FALSE}
# All features
fall <- select(adata,26:2566,9) ## full Alpha FT-IR spectral signatures + pH
pall <- select(adata,2567:2586,9) ## spectral principal components + pH

adata$pls <- predict(llist$pls, fall)
adata$glm <- predict(llist$glmnet, fall)
adata$xbl <- predict(llist$xgbLinear, fall)
adata$rfo <- predict(tlist$rf, pall)
adata$xbt <- predict(tlist$xgbTree, pall)
adata$cub <- predict(tlist$cubist, pall)
adata$st <- predict(st, adata)
```

Run and plot the quantile regression of Hp against the spectral ensemble predictions.

```{r}
stQ <- rq(Hp~st, tau=c(0.025,0.5,0.975), data = adata) ## quantile regression fit
print(stQ)
```

```{r, fig.align = "center", fig.cap = "Quantile regression fits of modeled vs observed reserve acidity (Hp). The blue lines are the 2.5% and 97.5% quantile regression estimates."}
par(pty="s", mar=c(4,4,1,1))
plot(Hp~st, xlab="Spectral ensemble Hp prediction", ylab="Measured Hp", cex.lab=1.3, 
     xlim=c(0,5), ylim=c(0,5), adata)
curve(stQ$coefficients[2]*x+stQ$coefficients[1], add=T, from=0, to=5, col="blue", lwd=2)
curve(stQ$coefficients[4]*x+stQ$coefficients[3], add=T, from=0, to=5, col="red", lwd=2)
curve(stQ$coefficients[6]*x+stQ$coefficients[5], add=T, from=0, to=5, col="blue", lwd=2)
abline(c(0,1), col="grey", lwd=1)
```

